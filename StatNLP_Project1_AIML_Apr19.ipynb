{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StatNLP_Project1_AIML_Apr19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "02CuH3nHbWWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDnd-rlMoJp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91cac205-1b5e-404f-dd63-d5c0ff905cb7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNfmAOt8nq90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRbpOF88bWWU",
        "colab_type": "code",
        "outputId": "95309b14-3b98-48b4-bb2f-2b49e7b87494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "df = pd.read_csv(\"blogtext.csv\")\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(681284, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>topic</th>\n",
              "      <th>sign</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>14,May,2004</td>\n",
              "      <td>Info has been found (+/- 100 pages,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>13,May,2004</td>\n",
              "      <td>These are the team members:   Drewe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>In het kader van kernfusie op aarde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>testing!!!  testing!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>11,June,2004</td>\n",
              "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ...                                               text\n",
              "0  2059027  ...             Info has been found (+/- 100 pages,...\n",
              "1  2059027  ...             These are the team members:   Drewe...\n",
              "2  2059027  ...             In het kader van kernfusie op aarde...\n",
              "3  2059027  ...                   testing!!!  testing!!!          \n",
              "4  3581210  ...               Thanks to Yahoo!'s Toolbar I can ...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjLROTw4bWWh",
        "colab_type": "text"
      },
      "source": [
        "#### 2.\tPreprocess rows of the “text” column (7.5 points)\n",
        "#### a.\tRemove unwanted characters\n",
        "#### b.\tConvert text to lowercase\n",
        "#### c.\tRemove unwanted spaces\n",
        "#### d.\tRemove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYqwZKixbWWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['text'] = df['text'].apply(lambda x : x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UpaYtPJbWWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "import nltk\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D95lYlqxbWWp",
        "colab_type": "code",
        "outputId": "62b052e7-6768-429e-bc4f-ad1752c6a550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "#removing stop words and unwanted characters/spaces \n",
        "for i in range(len(df['text'])):\n",
        "    df['text'][i] = re.sub(\"\\s\\s+\" , \" \", df['text'][i])\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-295eb81647f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\s\\s+\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1244\u001b[0m         \u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcacher_needs_updating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_with_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_maybe_update_cacher\u001b[0;34m(self, clear, verify_is_copy)\u001b[0m\n\u001b[1;32m   3346\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3347\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3348\u001b[0;31m                     \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cache_changed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcacher\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3349\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3350\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_maybe_cache_changed\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   3303\u001b[0m         \"\"\"The object has called back to us saying maybe it has changed.\n\u001b[1;32m   3304\u001b[0m         \"\"\"\n\u001b[0;32m-> 3305\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mblk_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m                 \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                 \u001b[0munfit_mgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, locs, values)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKkZAL3sbWWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['text'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mjgIb5ibWWw",
        "colab_type": "text"
      },
      "source": [
        "#### 3.\tAs we want to make this into a multi-label classification problem, you are required to merge all the label columns together, so that we have all the labels together for a particular sentence (7.5 points)\n",
        "#### a.\tLabel columns to merge: “gender”, “age”, “topic”, “sign”\n",
        "#### b.\tAfter completing the previous step, there should be only two columns in your data frame i.e. “text” and “labels”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VLhpsPmbWWx",
        "colab_type": "code",
        "outputId": "92b64b00-c2ff-49f2-df5d-f9413642f52b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.columns[1:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['gender', 'age', 'topic', 'sign'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnA3uIy6bWW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Label'] = df[df.columns[1:5]].apply(lambda x : \",\".join(x.dropna().astype('str')),axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzjLS_uzbWW5",
        "colab_type": "code",
        "outputId": "f1580a5f-eaff-4296-be04-8254b35cd7c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "df['Label'].head(10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   male,15,Student,Leo\n",
              "1                   male,15,Student,Leo\n",
              "2                   male,15,Student,Leo\n",
              "3                   male,15,Student,Leo\n",
              "4    male,33,InvestmentBanking,Aquarius\n",
              "5    male,33,InvestmentBanking,Aquarius\n",
              "6    male,33,InvestmentBanking,Aquarius\n",
              "7    male,33,InvestmentBanking,Aquarius\n",
              "8    male,33,InvestmentBanking,Aquarius\n",
              "9    male,33,InvestmentBanking,Aquarius\n",
              "Name: Label, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0GJFE1YbWW8",
        "colab_type": "code",
        "outputId": "96b169c4-f16a-4253-b63f-990d325407ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "df[:][['text','Label']].head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>info has been found (+/- 100 pages, and 4.5 m...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>these are the team members: drewes van der la...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>in het kader van kernfusie op aarde: maak je ...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>testing!!! testing!!!</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>thanks to yahoo!'s toolbar i can now 'capture...</td>\n",
              "      <td>male,33,InvestmentBanking,Aquarius</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                               Label\n",
              "0   info has been found (+/- 100 pages, and 4.5 m...                 male,15,Student,Leo\n",
              "1   these are the team members: drewes van der la...                 male,15,Student,Leo\n",
              "2   in het kader van kernfusie op aarde: maak je ...                 male,15,Student,Leo\n",
              "3                             testing!!! testing!!!                  male,15,Student,Leo\n",
              "4   thanks to yahoo!'s toolbar i can now 'capture...  male,33,InvestmentBanking,Aquarius"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4CcSGnbbWW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = df[:][['text','Label']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EamEAogbWXC",
        "colab_type": "text"
      },
      "source": [
        "#### 4.\tSeparate features and labels, and split the data into training and testing (5 points)\n",
        "#### 5.\tVectorize the features (5 points)\n",
        "#### a.\tCreate a Bag of Words using count vectorizer\n",
        "#### i.\tUse ngram_range=(1, 2)\n",
        "#### ii.\tVectorize training and testing features\n",
        "#### b.\tPrint the term-document matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGA8M5UNbWXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df2['text']\n",
        "Y = df2['Label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duZ6DAdDbWXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BSqn5i1bWXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru03KznubWXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a function that accepts a vectorizer and calculates the accuracy\n",
        "def tokenize_test(vect):\n",
        "    X_train_dtm = vect.fit_transform(X_train)\n",
        "    print('Features: ', X_train_dtm.shape[1])\n",
        "    X_test_dtm = vect.transform(X_test)\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(X_train_dtm, y_train)\n",
        "    #feature = nb.feature_count_\n",
        "    #print(\"NB feature shape\", feature)\n",
        "   # print(nb.feature_count_.shape)\n",
        "    y_train_pred = nb.predict(X_train_dtm)\n",
        "    y_pred_class = nb.predict(X_test_dtm)\n",
        "    print('Train Accuracy for NB : ', metrics.accuracy_score(y_train,y_train_pred))\n",
        "    print('Test Accuracy for NB: ', metrics.accuracy_score(y_test, y_pred_class))\n",
        "    logreg = LogisticRegression(C=1e9)\n",
        "    logreg.fit(X_train_dtm, y_train)\n",
        "    y_pred_class_LR = logreg.predict(X_test_dtm)\n",
        "#print(metrics.accuracy_score(y_test, y_pred_class))\n",
        "    y_train_LR = logreg.predict(X_train_dtm)\n",
        "    print('Train Accuracy for LR: ',metrics.accuracy_score(y_train, y_train_LR))\n",
        "    print('Test Accuracy for LR: ',metrics.accuracy_score(y_test, y_pred_class_LR))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1pWtGicbWXT",
        "colab_type": "code",
        "outputId": "bf2d6cf8-5117-47fd-bd62-1db46ad41f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "vector = CountVectorizer(ngram_range=(1, 2))\n",
        "tokenize_test(vector)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  13145454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrq_CjfabWXW",
        "colab_type": "code",
        "outputId": "16d04687-4a10-4f17-b3af-1466e1d0f51e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "X_train_dtm = vect.fit_transform(X_train)\n",
        "print('Features: ', X_train_dtm.shape[1])\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_dtm, y_train)\n",
        "    #feature = nb.feature_count_\n",
        "    #print(\"NB feature shape\", feature)\n",
        "   # print(nb.feature_count_.shape)\n",
        "y_train_pred = nb.predict(X_train_dtm)\n",
        "y_pred_class = nb.predict(X_test_dtm)\n",
        "print('Train Accuracy for NB : ', metrics.accuracy_score(y_train,y_train_pred))\n",
        "print('Test Accuracy for NB: ', metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1f90956d68c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_dtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Features: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_dtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test_dtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_dtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vect' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzg4Ik5HbWXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# features names\n",
        "features= vector.get_feature_names()\n",
        "print(features[20:80])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYyXs7Z6bWXb",
        "colab_type": "text"
      },
      "source": [
        "#### 6.\tCreate a dictionary to get the count of every label i.e. the key will be label name and value will be the total count of the label. Check below image for reference (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aaa5ScGbWXc",
        "colab_type": "code",
        "outputId": "cf516b2a-8bea-412c-ba54-51f0ff411c18",
        "colab": {}
      },
      "source": [
        "labels = {}\n",
        "labels = y_train.apply(lambda x : pd.value_counts(x.split(\",\"))).sum(axis = 0).to_dict()\n",
        "labels\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'indUnk': 2461.0,\n",
              " 'Sagittarius': 815.0,\n",
              " '24': 487.0,\n",
              " 'female': 3090.0,\n",
              " '34': 425.0,\n",
              " '27': 785.0,\n",
              " 'Education': 203.0,\n",
              " 'Aquarius': 432.0,\n",
              " '36': 1290.0,\n",
              " 'Aries': 3148.0,\n",
              " 'Fashion': 1218.0,\n",
              " 'male': 4410.0,\n",
              " '15': 455.0,\n",
              " 'Libra': 367.0,\n",
              " 'Student': 859.0,\n",
              " 'Capricorn': 160.0,\n",
              " '16': 320.0,\n",
              " '35': 1723.0,\n",
              " 'Technology': 1985.0,\n",
              " 'Taurus': 600.0,\n",
              " 'Pisces': 340.0,\n",
              " 'Sports-Recreation': 57.0,\n",
              " '17': 891.0,\n",
              " 'Leo': 230.0,\n",
              " 'Scorpio': 738.0,\n",
              " '39': 54.0,\n",
              " 'Virgo': 170.0,\n",
              " '26': 178.0,\n",
              " 'Engineering': 99.0,\n",
              " '23': 184.0,\n",
              " '13': 34.0,\n",
              " 'Cancer': 387.0,\n",
              " '14': 164.0,\n",
              " '25': 303.0,\n",
              " 'Science': 54.0,\n",
              " 'Communications-Media': 73.0,\n",
              " 'Gemini': 113.0,\n",
              " 'Marketing': 117.0,\n",
              " 'Automotive': 12.0,\n",
              " 'InvestmentBanking': 50.0,\n",
              " '33': 101.0,\n",
              " 'Banking': 13.0,\n",
              " '42': 12.0,\n",
              " 'Consulting': 17.0,\n",
              " '40': 1.0,\n",
              " 'Internet': 88.0,\n",
              " 'BusinessServices': 62.0,\n",
              " 'Law': 6.0,\n",
              " 'Arts': 35.0,\n",
              " '37': 25.0,\n",
              " '38': 33.0,\n",
              " '44': 3.0,\n",
              " 'Museums-Libraries': 13.0,\n",
              " '43': 4.0,\n",
              " 'Non-Profit': 59.0,\n",
              " 'LawEnforcement-Security': 7.0,\n",
              " '45': 11.0,\n",
              " '41': 12.0,\n",
              " 'HumanResources': 2.0,\n",
              " 'Publishing': 3.0,\n",
              " 'Religion': 4.0,\n",
              " '46': 5.0,\n",
              " 'Accounting': 3.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reFaDIM9bWXf",
        "colab_type": "text"
      },
      "source": [
        "#### 7.\t Transform the labels - (7.5 points)\n",
        "#### As we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose, it is convenient to use MultiLabelBinarizer from sklearn\n",
        "#### a.\tConvert your train and test labels using MultiLabelBinarizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "113jkz8UbWXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train_mlb = mlb.fit_transform(y_train)\n",
        "y_test_mlb = mlb.transform(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54Bdht02bWXj",
        "colab_type": "code",
        "outputId": "18d6bb6c-19c1-48d2-fd1e-0e9e6d094236",
        "colab": {}
      },
      "source": [
        "y_train_mlb[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "        0, 1, 1, 1, 1, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "        0, 1, 1, 1, 1, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
              "        1, 1, 1, 1, 1, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
              "        0, 1, 0, 1, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W94ZuHwlbWXm",
        "colab_type": "code",
        "outputId": "fe49b2ea-be01-4eef-e5fe-a646c2018943",
        "colab": {}
      },
      "source": [
        "y_test_mlb[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "        0, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "        0, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "        0, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "        0, 1, 1, 1, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vkH7vi_bWXp",
        "colab_type": "text"
      },
      "source": [
        "#### Choose a classifier - (5 points)\n",
        "#### In this task, we suggest using the One-vs-Rest approach, which is implemented in OneVsRestClassifier class. In this approach k classifiers (= number of tags) are trained. As a basic classifier, use LogisticRegression. It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time because the number of classifiers to train is large.\n",
        "#### a.\tUse a linear classifier of your choice, wrap it up in OneVsRestClassifier to train it on every label\n",
        "#### b.\tAs One-vs-Rest approach might not have been discussed in the sessions, we are providing you the code for that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xfu2CHDbWXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mav2cRtmbWXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LogisticRegression(solver = 'lbfgs',random_state= 123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xhf65nfbWXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = OneVsRestClassifier(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI04UGEUbWXz",
        "colab_type": "code",
        "outputId": "4cbc0de8-b597-4998-d969-dbbcd34b8200",
        "colab": {}
      },
      "source": [
        "feature_names = vector.get_feature_names()\n",
        "feature_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aa',\n",
              " 'aa amazing',\n",
              " 'aa anger',\n",
              " 'aa compared',\n",
              " 'aa keeps',\n",
              " 'aa nice',\n",
              " 'aa sd',\n",
              " 'aaa',\n",
              " 'aaa come',\n",
              " 'aaa coming',\n",
              " 'aaa discount',\n",
              " 'aaa rated',\n",
              " 'aaa tow',\n",
              " 'aaaa',\n",
              " 'aaaa jet',\n",
              " 'aaaaaaaaaaaah',\n",
              " 'aaaaaaaaaaahhhhhhhhhhhhhhhhhhh',\n",
              " 'aaaaaaaaaaahhhhhhhhhhhhhhhhhhh hw',\n",
              " 'aaaaaaaaah',\n",
              " 'aaaaaah',\n",
              " 'aaaaaawwwwww',\n",
              " 'aaaaaawwwwww gets',\n",
              " 'aaaaack',\n",
              " 'aaaaahhhh',\n",
              " 'aaaaahhhh heath',\n",
              " 'aaaah',\n",
              " 'aaaah wisdom',\n",
              " 'aaaahh',\n",
              " 'aaaarrrgghhhh',\n",
              " 'aaaarrrgghhhh slightly',\n",
              " 'aaah',\n",
              " 'aaah hafta',\n",
              " 'aaahhhh',\n",
              " 'aaahhhh diva',\n",
              " 'aaahing',\n",
              " 'aaahing mock',\n",
              " 'aaarrggghhh',\n",
              " 'aaarrggghhh plus',\n",
              " 'aaarrggghhh thanks',\n",
              " 'aaarrrggghhhhhhhhgggghhhhhh',\n",
              " 'aaarrrggghhhhhhhhgggghhhhhh dropped',\n",
              " 'aactually',\n",
              " 'aactually hope',\n",
              " 'aahed',\n",
              " 'aahed god',\n",
              " 'aaja',\n",
              " 'aaja unity',\n",
              " 'aal',\n",
              " 'aal eliminate',\n",
              " 'aal powerful',\n",
              " 'aal representative',\n",
              " 'aaldering',\n",
              " 'aaldering urllink',\n",
              " 'aar',\n",
              " 'aar toy',\n",
              " 'aarde',\n",
              " 'aarde maak',\n",
              " 'aargh',\n",
              " 'aargh told',\n",
              " 'aaron',\n",
              " 'aaron broke',\n",
              " 'aaron brown',\n",
              " 'aaron burr',\n",
              " 'aaron came',\n",
              " 'aaron club',\n",
              " 'aaron elandt',\n",
              " 'aaron friend',\n",
              " 'aaron gf',\n",
              " 'aaron heath',\n",
              " 'aaron hehe',\n",
              " 'aaron hi',\n",
              " 'aaron james',\n",
              " 'aaron know',\n",
              " 'aaron lacrosse',\n",
              " 'aaron like',\n",
              " 'aaron love',\n",
              " 'aaron making',\n",
              " 'aaron parents',\n",
              " 'aaron pops',\n",
              " 'aaron pulled',\n",
              " 'aaron realized',\n",
              " 'aaron rowand',\n",
              " 'aaron saying',\n",
              " 'aaron seeing',\n",
              " 'aaron sent',\n",
              " 'aaron sill',\n",
              " 'aaron stephen',\n",
              " 'aaron talladega',\n",
              " 'aaron today',\n",
              " 'aaron turned',\n",
              " 'aaron went',\n",
              " 'aarp',\n",
              " 'aarp considers',\n",
              " 'aarp got',\n",
              " 'aarrgghh',\n",
              " 'aarrgghh missed',\n",
              " 'aaugh',\n",
              " 'aaugh sun',\n",
              " 'ab',\n",
              " 'ab hoffman',\n",
              " 'ab reduction',\n",
              " 'ab workouts',\n",
              " 'aba',\n",
              " 'aba foundation',\n",
              " 'aba game',\n",
              " 'aba sessions',\n",
              " 'aba sleep',\n",
              " 'aba sus',\n",
              " 'aba teacher',\n",
              " 'aba therapy',\n",
              " 'aback',\n",
              " 'aback bit',\n",
              " 'aback blog',\n",
              " 'aback catholicism',\n",
              " 'aback coz',\n",
              " 'aback sure',\n",
              " 'aballaby',\n",
              " 'aballaby nbsp',\n",
              " 'abandon',\n",
              " 'abandon abandon',\n",
              " 'abandon elijah',\n",
              " 'abandon escape',\n",
              " 'abandon hope',\n",
              " 'abandon impotant',\n",
              " 'abandon life',\n",
              " 'abandon little',\n",
              " 'abandon program',\n",
              " 'abandon truth',\n",
              " 'abandon wrong',\n",
              " 'abandoned',\n",
              " 'abandoned adults',\n",
              " 'abandoned attacking',\n",
              " 'abandoned catholicism',\n",
              " 'abandoned drunk',\n",
              " 'abandoned kitten',\n",
              " 'abandoned pic',\n",
              " 'abandoned piers',\n",
              " 'abandoned players',\n",
              " 'abandoned read',\n",
              " 'abandoned ticket',\n",
              " 'abandoned ve',\n",
              " 'abandoning',\n",
              " 'abandoning faith',\n",
              " 'abandoning isreal',\n",
              " 'abandoning modest',\n",
              " 'abandoning wondering',\n",
              " 'abandonment',\n",
              " 'abandonment issues',\n",
              " 'abandonment tend',\n",
              " 'abandons',\n",
              " 'abandons hate',\n",
              " 'abandons knowing',\n",
              " 'abate',\n",
              " 'abate baby',\n",
              " 'abated',\n",
              " 'abated came',\n",
              " 'abba',\n",
              " 'abba tune',\n",
              " 'abbey',\n",
              " 'abbey jay',\n",
              " 'abbey trying',\n",
              " 'abbot',\n",
              " 'abbot alter',\n",
              " 'abbot blog',\n",
              " 'abbott',\n",
              " 'abbott going',\n",
              " 'abbott makes',\n",
              " 'abbreviate',\n",
              " 'abbreviate having',\n",
              " 'abbreviation',\n",
              " 'abbreviation film',\n",
              " 'abbreviations',\n",
              " 'abbreviations code',\n",
              " 'abby',\n",
              " 'abby hey',\n",
              " 'abby practically',\n",
              " 'abby sounds',\n",
              " 'abc',\n",
              " 'abc drama',\n",
              " 'abc keeping',\n",
              " 'abc news',\n",
              " 'abc night',\n",
              " 'abc nightline',\n",
              " 'abc probably',\n",
              " 'abc think',\n",
              " 'abcd',\n",
              " 'abcd student',\n",
              " 'abcfamily',\n",
              " 'abcfamily tapes',\n",
              " 'abcnews',\n",
              " 'abcnews com',\n",
              " 'abdicate',\n",
              " 'abdicate overthrown',\n",
              " 'abdo',\n",
              " 'abdo identified',\n",
              " 'abdomen',\n",
              " 'abdomen hips',\n",
              " 'abdomen thighs',\n",
              " 'abdominal',\n",
              " 'abdominal area',\n",
              " 'abducted',\n",
              " 'abducted weird',\n",
              " 'abduction',\n",
              " 'abduction tape',\n",
              " 'abductor',\n",
              " 'abductor isn',\n",
              " 'abdul',\n",
              " 'abdul head',\n",
              " 'abdul rahman',\n",
              " 'abe',\n",
              " 'abe called',\n",
              " 'abe fuck',\n",
              " 'abe lincoln',\n",
              " 'abegeeeeee',\n",
              " 'abegeeeeee abiis',\n",
              " 'abercrombie',\n",
              " 'abercrombie beliefs',\n",
              " 'abercrombie boxers',\n",
              " 'abercrombie chick',\n",
              " 'abercrombie cologne',\n",
              " 'abercrombie fags',\n",
              " 'abercrombie shit',\n",
              " 'abercrombie underwear',\n",
              " 'aberrant',\n",
              " 'aberrant behavior',\n",
              " 'aberrant start',\n",
              " 'aberration',\n",
              " 'aberration got',\n",
              " 'abhor',\n",
              " 'abhor middle',\n",
              " 'abhor remember',\n",
              " 'abhorred',\n",
              " 'abhorred avoided',\n",
              " 'abi',\n",
              " 'abi station',\n",
              " 'abide',\n",
              " 'abide add',\n",
              " 'abide geneva',\n",
              " 'abided',\n",
              " 'abided international',\n",
              " 'abiding',\n",
              " 'abiding christian',\n",
              " 'abiding citizens',\n",
              " 'abiding pedestrians',\n",
              " 'abidjan',\n",
              " 'abidjan ivory',\n",
              " 'abiis',\n",
              " 'abiis deh',\n",
              " 'abilities',\n",
              " 'abilities applys',\n",
              " 'abilities doubting',\n",
              " 'abilities eminiem',\n",
              " 'abilities increased',\n",
              " 'abilities limited',\n",
              " 'abilities notary',\n",
              " 'abilities pretty',\n",
              " 'abilities saying',\n",
              " 'abilities spiderman',\n",
              " 'abilities staff',\n",
              " 'abilities stretching',\n",
              " 'abilities talk',\n",
              " 'abilities trustworthy',\n",
              " 'abilities wouldn',\n",
              " 'ability',\n",
              " 'ability achieve',\n",
              " 'ability agencies',\n",
              " 'ability analyze',\n",
              " 'ability ash',\n",
              " 'ability asked',\n",
              " 'ability audience',\n",
              " 'ability believe',\n",
              " 'ability cast',\n",
              " 'ability catch',\n",
              " 'ability charm',\n",
              " 'ability cheer',\n",
              " 'ability collect',\n",
              " 'ability connect',\n",
              " 'ability control',\n",
              " 'ability create',\n",
              " 'ability differently',\n",
              " 'ability don',\n",
              " 'ability draw',\n",
              " 'ability edit',\n",
              " 'ability express',\n",
              " 'ability extra',\n",
              " 'ability extract',\n",
              " 'ability fight',\n",
              " 'ability fire',\n",
              " 'ability fortunate',\n",
              " 'ability function',\n",
              " 'ability generate',\n",
              " 'ability great',\n",
              " 'ability hold',\n",
              " 'ability incorporate',\n",
              " 'ability intellect',\n",
              " 'ability laugh',\n",
              " 'ability lead',\n",
              " 'ability mask',\n",
              " 'ability melodiously',\n",
              " 'ability motivation',\n",
              " 'ability new',\n",
              " 'ability overly',\n",
              " 'ability owner',\n",
              " 'ability pay',\n",
              " 'ability perspective',\n",
              " 'ability place',\n",
              " 'ability power',\n",
              " 'ability practice',\n",
              " 'ability program',\n",
              " 'ability raise',\n",
              " 'ability repeat',\n",
              " 'ability reservation',\n",
              " 'ability rights',\n",
              " 'ability said',\n",
              " 'ability searching',\n",
              " 'ability seemingly',\n",
              " 'ability set',\n",
              " 'ability shine',\n",
              " 'ability sit',\n",
              " 'ability sleep',\n",
              " 'ability stand',\n",
              " 'ability strive',\n",
              " 'ability talking',\n",
              " 'ability tap',\n",
              " 'ability think',\n",
              " 'ability toplace',\n",
              " 'ability touch',\n",
              " 'ability understand',\n",
              " 'ability use',\n",
              " 'ability view',\n",
              " 'ability walk',\n",
              " 'ability work',\n",
              " 'ability write',\n",
              " 'abiss',\n",
              " 'abiss ituu',\n",
              " 'abit',\n",
              " 'abit nervous',\n",
              " 'abit passing',\n",
              " 'abject',\n",
              " 'abject horror',\n",
              " 'ablaze',\n",
              " 'ablaze lot',\n",
              " 'able',\n",
              " 'able access',\n",
              " 'able accommodate',\n",
              " 'able actually',\n",
              " 'able add',\n",
              " 'able advantage',\n",
              " 'able afford',\n",
              " 'able answer',\n",
              " 'able arrange',\n",
              " 'able ascertain',\n",
              " 'able ask',\n",
              " 'able assist',\n",
              " 'able attend',\n",
              " 'able avail',\n",
              " 'able avoid',\n",
              " 'able basis',\n",
              " 'able battle',\n",
              " 'able belt',\n",
              " 'able best',\n",
              " 'able blog',\n",
              " 'able blow',\n",
              " 'able brainwash',\n",
              " 'able bring',\n",
              " 'able broaden',\n",
              " 'able busy',\n",
              " 'able buy',\n",
              " 'able caddy',\n",
              " 'able camera',\n",
              " 'able car',\n",
              " 'able carry',\n",
              " 'able cased',\n",
              " 'able catch',\n",
              " 'able cavite',\n",
              " 'able change',\n",
              " 'able character',\n",
              " 'able charm',\n",
              " 'able check',\n",
              " 'able chemlab',\n",
              " 'able chris',\n",
              " 'able click',\n",
              " 'able close',\n",
              " 'able clover',\n",
              " 'able come',\n",
              " 'able comfortably',\n",
              " 'able communicate',\n",
              " 'able complete',\n",
              " 'able computer',\n",
              " 'able conclusively',\n",
              " 'able conduct',\n",
              " 'able confirm',\n",
              " 'able connect',\n",
              " 'able consider',\n",
              " 'able control',\n",
              " 'able convert',\n",
              " 'able copy',\n",
              " 'able cos',\n",
              " 'able create',\n",
              " 'able day',\n",
              " 'able deal',\n",
              " 'able decide',\n",
              " 'able deliver',\n",
              " 'able depend',\n",
              " 'able digest',\n",
              " 'able dinner',\n",
              " 'able distinguish',\n",
              " 'able dominate',\n",
              " 'able download',\n",
              " 'able drain',\n",
              " 'able drift',\n",
              " 'able drink',\n",
              " 'able drive',\n",
              " 'able drop',\n",
              " 'able duck',\n",
              " 'able dukakisize',\n",
              " 'able dunk',\n",
              " 'able easily',\n",
              " 'able elf',\n",
              " 'able end',\n",
              " 'able energy',\n",
              " 'able engage',\n",
              " 'able entertain',\n",
              " 'able establish',\n",
              " 'able eventually',\n",
              " 'able exit',\n",
              " 'able expose',\n",
              " 'able express',\n",
              " 'able extended',\n",
              " 'able eye',\n",
              " 'able eyes',\n",
              " 'able face',\n",
              " 'able fall',\n",
              " 'able fast',\n",
              " 'able father',\n",
              " 'able favorite',\n",
              " 'able feel',\n",
              " 'able female',\n",
              " 'able fight',\n",
              " 'able figure',\n",
              " 'able find',\n",
              " 'able fit',\n",
              " 'able fly',\n",
              " 'able focus',\n",
              " 'able forget',\n",
              " 'able forward',\n",
              " 'able fully',\n",
              " 'able fun',\n",
              " 'able function',\n",
              " 'able gather',\n",
              " 'able generate',\n",
              " 'able glorious',\n",
              " 'able golf',\n",
              " 'able goof',\n",
              " 'able got',\n",
              " 'able govern',\n",
              " 'able grab',\n",
              " 'able grace',\n",
              " 'able greater',\n",
              " 'able greedy',\n",
              " 'able handle',\n",
              " 'able hang',\n",
              " 'able hard',\n",
              " 'able hate',\n",
              " 'able hauled',\n",
              " 'able hear',\n",
              " 'able help',\n",
              " 'able hide',\n",
              " 'able hit',\n",
              " 'able home',\n",
              " 'able hop',\n",
              " 'able house',\n",
              " 'able imbue',\n",
              " 'able impact',\n",
              " 'able increase',\n",
              " 'able influence',\n",
              " 'able inundate',\n",
              " 'able join',\n",
              " 'able kick',\n",
              " 'able kim',\n",
              " 'able know',\n",
              " 'able large',\n",
              " 'able laugh',\n",
              " 'able leap',\n",
              " 'able learn',\n",
              " 'able leave',\n",
              " 'able let',\n",
              " 'able liberate',\n",
              " 'able lie',\n",
              " 'able life',\n",
              " 'able lift',\n",
              " 'able like',\n",
              " 'able little',\n",
              " 'able locate',\n",
              " 'able london',\n",
              " 'able look',\n",
              " 'able looking',\n",
              " 'able lot',\n",
              " 'able love',\n",
              " 'able lug',\n",
              " 'able mail',\n",
              " 'able maintain',\n",
              " 'able makes',\n",
              " 'able maneuver',\n",
              " 'able master',\n",
              " 'able maybe',\n",
              " 'able meet',\n",
              " 'able mess',\n",
              " 'able mile',\n",
              " 'able mix',\n",
              " 'able moved',\n",
              " 'able movie',\n",
              " 'able muster',\n",
              " 'able negotiate',\n",
              " 'able occasionally',\n",
              " 'able offer',\n",
              " 'able official',\n",
              " 'able oh',\n",
              " 'able operate',\n",
              " 'able optimist',\n",
              " 'able overcome',\n",
              " 'able paint',\n",
              " 'able participate',\n",
              " 'able pay',\n",
              " 'able people',\n",
              " 'able permanent',\n",
              " 'able person',\n",
              " 'able pick',\n",
              " 'able pictures',\n",
              " 'able place',\n",
              " 'able play',\n",
              " 'able post',\n",
              " 'able practices',\n",
              " 'able pretty',\n",
              " 'able profit',\n",
              " 'able prove',\n",
              " 'able proverbial',\n",
              " 'able pull',\n",
              " 'able push',\n",
              " 'able quickly',\n",
              " 'able raed',\n",
              " 'able reach',\n",
              " 'able react',\n",
              " 'able read',\n",
              " 'able realize',\n",
              " 'able recognise',\n",
              " 'able rectify',\n",
              " 'able redefine',\n",
              " 'able reduce',\n",
              " 'able relate',\n",
              " 'able relative',\n",
              " 'able relax',\n",
              " 'able release',\n",
              " 'able replace',\n",
              " 'able replaced',\n",
              " 'able report',\n",
              " 'able represent',\n",
              " 'able resist',\n",
              " 'able ride',\n",
              " 'able run',\n",
              " 'able said',\n",
              " 'able salvage',\n",
              " 'able save',\n",
              " 'able sch',\n",
              " 'able separate',\n",
              " 'able share',\n",
              " 'able shim',\n",
              " 'able shut',\n",
              " 'able sing',\n",
              " 'able site',\n",
              " 'able sleep',\n",
              " 'able small',\n",
              " 'able snatch',\n",
              " 'able sneak',\n",
              " 'able soak',\n",
              " 'able someday',\n",
              " 'able song',\n",
              " 'able songs',\n",
              " 'able speak',\n",
              " 'able spend',\n",
              " 'able stay',\n",
              " 'able step',\n",
              " 'able steps',\n",
              " 'able stick',\n",
              " 'able stop',\n",
              " 'able stroll',\n",
              " 'able supposed',\n",
              " 'able sure',\n",
              " 'able swim',\n",
              " 'able takes',\n",
              " 'able talk',\n",
              " 'able tart',\n",
              " 'able taste',\n",
              " 'able teach',\n",
              " 'able tell',\n",
              " 'able test',\n",
              " 'able thing',\n",
              " 'able things',\n",
              " 'able think',\n",
              " 'able tickets',\n",
              " 'able time',\n",
              " 'able tom',\n",
              " 'able tonight',\n",
              " 'able touch',\n",
              " 'able transfer',\n",
              " 'able travel',\n",
              " 'able trust',\n",
              " 'able turn',\n",
              " 'able type',\n",
              " 'able ugh',\n",
              " 'able understand',\n",
              " 'able update',\n",
              " 'able urllink',\n",
              " 'able use',\n",
              " 'able utr',\n",
              " 'able vancouver',\n",
              " 'able vote',\n",
              " 'able wake',\n",
              " 'able walk',\n",
              " 'able want',\n",
              " 'able watch',\n",
              " 'able wax',\n",
              " 'able week',\n",
              " 'able wirelessly',\n",
              " 'able withstand',\n",
              " 'able work',\n",
              " 'able wrap',\n",
              " 'able write',\n",
              " 'able wrong',\n",
              " 'able year',\n",
              " 'able yell',\n",
              " 'able yoga',\n",
              " 'ablity',\n",
              " 'ablity transportation',\n",
              " 'ablum',\n",
              " 'ablum reflection',\n",
              " 'abm',\n",
              " 'abm radars',\n",
              " 'abm treaty',\n",
              " 'abnormal',\n",
              " 'abnormal positions',\n",
              " 'abnormal screwed',\n",
              " 'abnormal stress',\n",
              " 'aboard',\n",
              " 'aboard bb',\n",
              " 'aboard boat',\n",
              " 'aboard loved',\n",
              " 'aboard powerless',\n",
              " 'aboard screenwriter',\n",
              " 'abode',\n",
              " 'abode wahoo',\n",
              " 'abolition',\n",
              " 'abolition primary',\n",
              " 'abomination',\n",
              " 'abomination enemies',\n",
              " 'abomination winter',\n",
              " 'aboot',\n",
              " 'aboot nbsp',\n",
              " 'aboout',\n",
              " 'aboout clarinet',\n",
              " 'abort',\n",
              " 'abort abort',\n",
              " 'abort longer',\n",
              " 'abort took',\n",
              " 'aborted',\n",
              " 'aborted fair',\n",
              " 'aborted highly',\n",
              " 'aborted reason',\n",
              " 'aborted song',\n",
              " 'abortion',\n",
              " 'abortion akin',\n",
              " 'abortion edwards',\n",
              " 'abortion environment',\n",
              " 'abortion issue',\n",
              " 'abortion like',\n",
              " 'abortion matter',\n",
              " 'abortion problem',\n",
              " 'abortion procedure',\n",
              " 'abortion shows',\n",
              " 'abortions',\n",
              " 'abortions past',\n",
              " 'abortions void',\n",
              " 'aborts',\n",
              " 'aborts idea',\n",
              " 'abotu',\n",
              " 'abotu kids',\n",
              " 'abou',\n",
              " 'abound',\n",
              " 'abound like',\n",
              " 'abound new',\n",
              " 'abound sites',\n",
              " 'abounds',\n",
              " 'abounds sunshine',\n",
              " 'abouta',\n",
              " 'abouta shop',\n",
              " 'aboutinternational',\n",
              " 'aboutinternational relations',\n",
              " 'aboutyou',\n",
              " 'aboutyou talking',\n",
              " 'abovei',\n",
              " 'abovei close',\n",
              " 'abovementioned',\n",
              " 'abovementioned late',\n",
              " 'abovementioned nation',\n",
              " 'aboy',\n",
              " 'aboy blonde',\n",
              " 'abraham',\n",
              " 'abraham lincoln',\n",
              " 'abraham maslow',\n",
              " 'abrams',\n",
              " 'abrams created',\n",
              " 'abrasions',\n",
              " 'abrasions road',\n",
              " 'abrasive',\n",
              " 'abrasive complicated',\n",
              " 'abreast',\n",
              " 'abreast current',\n",
              " 'abreast going',\n",
              " 'abreu',\n",
              " 'abreu phi',\n",
              " 'abri',\n",
              " 'abri el',\n",
              " 'abridged',\n",
              " 'abridged script',\n",
              " 'abridging',\n",
              " 'abridging freedom',\n",
              " 'abrio',\n",
              " 'abrio la',\n",
              " 'abroad',\n",
              " 'abroad attorney',\n",
              " 'abroad better',\n",
              " 'abroad bordeaux',\n",
              " 'abroad drinks',\n",
              " 'abroad failed',\n",
              " 'abroad falls',\n",
              " 'abroad friends',\n",
              " 'abroad integrity',\n",
              " 'abroad issue',\n",
              " 'abroad justified',\n",
              " 'abroad position',\n",
              " 'abroad sophomore',\n",
              " 'abroad yes',\n",
              " 'abrogation',\n",
              " 'abrogation principles',\n",
              " 'abrupt',\n",
              " 'abrupt click',\n",
              " 'abrupt end',\n",
              " 'abrupt times',\n",
              " 'abruptly',\n",
              " 'abruptly asked',\n",
              " 'abruptly interrupt',\n",
              " 'abruptly want',\n",
              " 'abs',\n",
              " 'abs bland',\n",
              " 'abs favorite',\n",
              " 'abs love',\n",
              " 'abs maybe',\n",
              " 'abs spy',\n",
              " 'abscess',\n",
              " 'abscess bank',\n",
              " 'abscond',\n",
              " 'abscond need',\n",
              " 'absence',\n",
              " 'absence benefit',\n",
              " 'absence certain',\n",
              " 'absence cheers',\n",
              " 'absence consumed',\n",
              " 'absence controversial',\n",
              " 'absence death',\n",
              " 'absence defintely',\n",
              " 'absence democratic',\n",
              " 'absence experience',\n",
              " 'absence football',\n",
              " 'absence good',\n",
              " 'absence hillbilly',\n",
              " 'absence linked',\n",
              " 'absence makes',\n",
              " 'absence memorable',\n",
              " 'absence nbsp',\n",
              " 'absence pits',\n",
              " 'absence reproached',\n",
              " 'absence spam',\n",
              " 'absence specific',\n",
              " 'absent',\n",
              " 'absent arguments',\n",
              " 'absent better',\n",
              " 'absent bullies',\n",
              " 'absent hate',\n",
              " 'absent regions',\n",
              " 'absent think',\n",
              " 'absent today',\n",
              " 'absent vacuum',\n",
              " 'absent weeks',\n",
              " 'absentee',\n",
              " 'absentee ballots',\n",
              " 'absentee votes',\n",
              " 'absenteeism',\n",
              " 'absenteeism taking',\n",
              " 'absentia',\n",
              " 'absentia new',\n",
              " 'absoluetley',\n",
              " 'absoluetley way',\n",
              " 'absolulty',\n",
              " 'absolulty clue',\n",
              " 'absolulty past',\n",
              " 'absolut',\n",
              " 'absolut ketel',\n",
              " 'absolute',\n",
              " 'absolute asked',\n",
              " 'absolute barrier',\n",
              " 'absolute best',\n",
              " 'absolute blast',\n",
              " 'absolute bliss',\n",
              " 'absolute blockbuster',\n",
              " 'absolute complete',\n",
              " 'absolute crap',\n",
              " 'absolute declarative',\n",
              " 'absolute fabulous',\n",
              " 'absolute fairplay',\n",
              " 'absolute hell',\n",
              " 'absolute hoot',\n",
              " 'absolute horror',\n",
              " 'absolute idiot',\n",
              " 'absolute moxie',\n",
              " 'absolute nightmare',\n",
              " 'absolute opposition',\n",
              " 'absolute pure',\n",
              " 'absolute resort',\n",
              " 'absolute shit',\n",
              " 'absolute shite',\n",
              " 'absolute source',\n",
              " 'absolute spiritual',\n",
              " 'absolute success',\n",
              " 'absolute truth',\n",
              " 'absolute whooped',\n",
              " 'absolutely',\n",
              " 'absolutely abhor',\n",
              " 'absolutely add',\n",
              " 'absolutely admittedly',\n",
              " 'absolutely adorable',\n",
              " 'absolutely adore',\n",
              " 'absolutely agree',\n",
              " 'absolutely amazing',\n",
              " 'absolutely appalled',\n",
              " 'absolutely attention',\n",
              " 'absolutely awesome',\n",
              " 'absolutely beautiful',\n",
              " 'absolutely believe',\n",
              " 'absolutely benefit',\n",
              " 'absolutely blown',\n",
              " 'absolutely bubble',\n",
              " 'absolutely bubbles',\n",
              " 'absolutely certain',\n",
              " 'absolutely chance',\n",
              " 'absolutely change',\n",
              " 'absolutely common',\n",
              " 'absolutely complex',\n",
              " 'absolutely condescencion',\n",
              " 'absolutely confidence',\n",
              " 'absolutely crazy',\n",
              " 'absolutely damn',\n",
              " 'absolutely delicious',\n",
              " 'absolutely desire',\n",
              " 'absolutely different',\n",
              " 'absolutely direction',\n",
              " 'absolutely dirty',\n",
              " 'absolutely disrespectful',\n",
              " 'absolutely dogged',\n",
              " 'absolutely don',\n",
              " 'absolutely dreading',\n",
              " 'absolutely duf',\n",
              " 'absolutely english',\n",
              " 'absolutely exposed',\n",
              " 'absolutely fabulous',\n",
              " 'absolutely false',\n",
              " 'absolutely fantastic',\n",
              " 'absolutely feel',\n",
              " 'absolutely fits',\n",
              " 'absolutely food',\n",
              " 'absolutely forbid',\n",
              " 'absolutely friend',\n",
              " 'absolutely going',\n",
              " 'absolutely gorgeous',\n",
              " 'absolutely guess',\n",
              " 'absolutely happy',\n",
              " 'absolutely hate',\n",
              " 'absolutely hated',\n",
              " 'absolutely hating',\n",
              " 'absolutely highlighters',\n",
              " 'absolutely hilarious',\n",
              " 'absolutely home',\n",
              " 'absolutely horrifies',\n",
              " 'absolutely idea',\n",
              " 'absolutely immediate',\n",
              " 'absolutely included',\n",
              " 'absolutely incredible',\n",
              " 'absolutely know',\n",
              " 'absolutely knowing',\n",
              " 'absolutely left',\n",
              " 'absolutely life',\n",
              " 'absolutely loathe',\n",
              " 'absolutely love',\n",
              " 'absolutely loveable',\n",
              " 'absolutely luck',\n",
              " 'absolutely maddening',\n",
              " 'absolutely marvelous',\n",
              " 'absolutely mind',\n",
              " 'absolutely moving',\n",
              " 'absolutely msnbc',\n",
              " 'absolutely necessary',\n",
              " 'absolutely nice',\n",
              " 'absolutely non',\n",
              " 'absolutely note',\n",
              " 'absolutely nuts',\n",
              " 'absolutely obsessive',\n",
              " 'absolutely obviously',\n",
              " 'absolutely offense',\n",
              " 'absolutely open',\n",
              " 'absolutely ordinary',\n",
              " 'absolutely palast',\n",
              " 'absolutely perfect',\n",
              " 'absolutely perfectly',\n",
              " 'absolutely pissed',\n",
              " 'absolutely poems',\n",
              " 'absolutely positive',\n",
              " 'absolutely positively',\n",
              " 'absolutely problem',\n",
              " 'absolutely radiant',\n",
              " 'absolutely rankings',\n",
              " 'absolutely reason',\n",
              " 'absolutely refreshing',\n",
              " 'absolutely refuse',\n",
              " 'absolutely refused',\n",
              " 'absolutely relaxing',\n",
              " 'absolutely ridiculous',\n",
              " 'absolutely right',\n",
              " 'absolutely rocked',\n",
              " 'absolutely salute',\n",
              " 'absolutely sense',\n",
              " 'absolutely service',\n",
              " 'absolutely sign',\n",
              " 'absolutely smelly',\n",
              " 'absolutely spark',\n",
              " 'absolutely splash',\n",
              " 'absolutely sports',\n",
              " 'absolutely spotless',\n",
              " 'absolutely stand',\n",
              " 'absolutely stopped',\n",
              " 'absolutely stunning',\n",
              " 'absolutely stupid',\n",
              " 'absolutely sublime',\n",
              " 'absolutely terrified',\n",
              " 'absolutely think',\n",
              " 'absolutely time',\n",
              " 'absolutely tons',\n",
              " 'absolutely traumatized',\n",
              " 'absolutely true',\n",
              " 'absolutely try',\n",
              " 'absolutely ucalled',\n",
              " 'absolutely use',\n",
              " 'absolutely weekend',\n",
              " 'absolutely women',\n",
              " 'absolutes',\n",
              " 'absolutes attempted',\n",
              " 'absolutes life',\n",
              " 'absolutes like',\n",
              " 'absolutes matter',\n",
              " 'absolutes objective',\n",
              " 'absolutes peace',\n",
              " 'absolutes possibility',\n",
              " 'absolutes subjective',\n",
              " 'absolution',\n",
              " 'absolution agnostic',\n",
              " 'absolution couldnt',\n",
              " 'absolution key',\n",
              " 'absolution looking',\n",
              " 'absolution muse',\n",
              " 'absolution singing',\n",
              " 'absolutist',\n",
              " 'absolutist taste',\n",
              " 'absolutistic',\n",
              " 'absolutley',\n",
              " 'absolutley amazing',\n",
              " 'absolutley random',\n",
              " 'absolutly',\n",
              " 'absolutly courage',\n",
              " 'absolutly crazy',\n",
              " 'absolutly fucking',\n",
              " 'absolutly love',\n",
              " 'absolutly pointless',\n",
              " 'absolutly soaked',\n",
              " 'absolutly think',\n",
              " 'absolutly world',\n",
              " 'absolved',\n",
              " 'absolved responsibility',\n",
              " 'absolving',\n",
              " 'absolving child',\n",
              " 'absolving parents',\n",
              " 'absorb',\n",
              " 'absorb atmosphere',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX7F7VfnbWX3",
        "colab_type": "text"
      },
      "source": [
        "####  9.\tFit the classifier, make predictions and get the accuracy (5 points)\n",
        "####  a.\tPrint the following\n",
        "#### i.\tAccuracy score\n",
        "#### ii.\tF1 score\n",
        "#### iii.\tAverage precision score\n",
        "#### iv.\tAverage recall score\n",
        "#### v.\tTip: Make sure you are familiar with all of them. How would you expect the things to work for the multi-label scenario? Read about micro/macro/weighted averaging\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV1a0peZbWX4",
        "colab_type": "code",
        "outputId": "915aecfc-ac64-4f66-b200-bb6f4e11ce76",
        "colab": {}
      },
      "source": [
        "clf.fit(X_train_dtm,y_train_mlb)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label 0 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label 29 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label 33 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label 39 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label 40 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
              "                                                 dual=False, fit_intercept=True,\n",
              "                                                 intercept_scaling=1,\n",
              "                                                 l1_ratio=None, max_iter=100,\n",
              "                                                 multi_class='warn',\n",
              "                                                 n_jobs=None, penalty='l2',\n",
              "                                                 random_state=123,\n",
              "                                                 solver='lbfgs', tol=0.0001,\n",
              "                                                 verbose=0, warm_start=False),\n",
              "                    n_jobs=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYtaytyVbWX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_clf = clf.predict(X_test_dtm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC9GDAXbbWX-",
        "colab_type": "code",
        "outputId": "1a2ac13f-f152-46f9-8fcd-7104c28bb128",
        "colab": {}
      },
      "source": [
        "metrics.accuracy_score(y_test_mlb,y_pred_clf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2412"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL2c-baybWYB",
        "colab_type": "code",
        "outputId": "468974df-4535-4854-c57c-f5d22615f494",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "classification_report(y_test_mlb, y_pred_clf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      2500\\n           1       0.67      0.09      0.16        68\\n           2       0.00      0.00      0.00         0\\n           3       0.87      0.50      0.64       625\\n           4       0.72      0.37      0.49       647\\n           5       0.81      0.76      0.79      1298\\n           6       0.84      0.36      0.50       363\\n           7       0.75      0.51      0.61       827\\n           8       0.88      0.44      0.59       596\\n           9       0.77      0.30      0.43       571\\n          10       1.00      0.15      0.27        13\\n          11       1.00      0.04      0.08        25\\n          12       0.81      0.73      0.77      1192\\n          13       1.00      0.23      0.38        52\\n          14       0.81      0.29      0.43       202\\n          15       0.75      0.09      0.16        98\\n          16       0.96      0.50      0.65       404\\n          17       0.00      0.00      0.00        37\\n          18       0.00      0.00      0.00         0\\n          19       0.91      0.20      0.33        50\\n          20       0.76      0.14      0.23       202\\n          21       0.95      0.26      0.41        69\\n          22       0.00      0.00      0.00        12\\n          23       0.89      0.24      0.38       127\\n          24       1.00      0.21      0.35        28\\n          25       0.80      0.50      0.62       794\\n          26       0.79      0.60      0.68       883\\n          27       0.79      0.41      0.54       826\\n          28       0.92      0.18      0.30        66\\n          29       1.00      1.00      1.00      2500\\n          30       0.73      0.12      0.21       129\\n          31       0.77      0.73      0.75      1233\\n          32       0.85      0.63      0.72      1197\\n          33       1.00      1.00      1.00      2500\\n          34       0.84      0.58      0.69      1009\\n          35       0.79      0.58      0.67      1099\\n          36       0.82      0.72      0.77      1074\\n          37       0.97      1.00      0.98      2427\\n          38       0.79      0.44      0.56       888\\n          39       1.00      1.00      1.00      2500\\n          40       1.00      1.00      1.00      2500\\n          41       0.99      1.00      1.00      2482\\n          42       0.81      0.87      0.84      1572\\n          43       0.83      0.22      0.35       289\\n          44       0.80      0.24      0.37       139\\n          45       0.93      0.99      0.96      2291\\n          46       0.86      0.92      0.88      1857\\n          47       0.78      0.49      0.60       758\\n          48       0.80      0.50      0.61       917\\n          49       1.00      0.20      0.33        51\\n          50       0.00      0.00      0.00         8\\n          51       0.83      0.50      0.62       672\\n\\n   micro avg       0.91      0.78      0.84     42667\\n   macro avg       0.78      0.46      0.53     42667\\nweighted avg       0.89      0.78      0.81     42667\\n samples avg       0.91      0.78      0.83     42667\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMDTW7wdbWYE",
        "colab_type": "code",
        "outputId": "d0511a0d-21db-402a-88d2-12fd412009aa",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_test_mlb, y_pred_clf))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2500\n",
            "           1       0.67      0.09      0.16        68\n",
            "           2       0.00      0.00      0.00         0\n",
            "           3       0.87      0.50      0.64       625\n",
            "           4       0.72      0.37      0.49       647\n",
            "           5       0.81      0.76      0.79      1298\n",
            "           6       0.84      0.36      0.50       363\n",
            "           7       0.75      0.51      0.61       827\n",
            "           8       0.88      0.44      0.59       596\n",
            "           9       0.77      0.30      0.43       571\n",
            "          10       1.00      0.15      0.27        13\n",
            "          11       1.00      0.04      0.08        25\n",
            "          12       0.81      0.73      0.77      1192\n",
            "          13       1.00      0.23      0.38        52\n",
            "          14       0.81      0.29      0.43       202\n",
            "          15       0.75      0.09      0.16        98\n",
            "          16       0.96      0.50      0.65       404\n",
            "          17       0.00      0.00      0.00        37\n",
            "          18       0.00      0.00      0.00         0\n",
            "          19       0.91      0.20      0.33        50\n",
            "          20       0.76      0.14      0.23       202\n",
            "          21       0.95      0.26      0.41        69\n",
            "          22       0.00      0.00      0.00        12\n",
            "          23       0.89      0.24      0.38       127\n",
            "          24       1.00      0.21      0.35        28\n",
            "          25       0.80      0.50      0.62       794\n",
            "          26       0.79      0.60      0.68       883\n",
            "          27       0.79      0.41      0.54       826\n",
            "          28       0.92      0.18      0.30        66\n",
            "          29       1.00      1.00      1.00      2500\n",
            "          30       0.73      0.12      0.21       129\n",
            "          31       0.77      0.73      0.75      1233\n",
            "          32       0.85      0.63      0.72      1197\n",
            "          33       1.00      1.00      1.00      2500\n",
            "          34       0.84      0.58      0.69      1009\n",
            "          35       0.79      0.58      0.67      1099\n",
            "          36       0.82      0.72      0.77      1074\n",
            "          37       0.97      1.00      0.98      2427\n",
            "          38       0.79      0.44      0.56       888\n",
            "          39       1.00      1.00      1.00      2500\n",
            "          40       1.00      1.00      1.00      2500\n",
            "          41       0.99      1.00      1.00      2482\n",
            "          42       0.81      0.87      0.84      1572\n",
            "          43       0.83      0.22      0.35       289\n",
            "          44       0.80      0.24      0.37       139\n",
            "          45       0.93      0.99      0.96      2291\n",
            "          46       0.86      0.92      0.88      1857\n",
            "          47       0.78      0.49      0.60       758\n",
            "          48       0.80      0.50      0.61       917\n",
            "          49       1.00      0.20      0.33        51\n",
            "          50       0.00      0.00      0.00         8\n",
            "          51       0.83      0.50      0.62       672\n",
            "\n",
            "   micro avg       0.91      0.78      0.84     42667\n",
            "   macro avg       0.78      0.46      0.53     42667\n",
            "weighted avg       0.89      0.78      0.81     42667\n",
            " samples avg       0.91      0.78      0.83     42667\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RS8tFf7bWYH",
        "colab_type": "code",
        "outputId": "0bf08a8b-ce04-4350-b738-f8317f7be7fd",
        "colab": {}
      },
      "source": [
        "metrics.average_precision_score(y_test_mlb, y_pred_clf,average='micro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7786164795833284"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j50ar2WtbWYK",
        "colab_type": "code",
        "outputId": "a288e776-426a-43e6-d86e-9a83e912475d",
        "colab": {}
      },
      "source": [
        "#average='micro\n",
        "metrics.recall_score(y_test_mlb, y_pred_clf,average='micro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7765720580307967"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70pBCITIbWYR",
        "colab_type": "code",
        "outputId": "fa273373-2702-4f38-b942-54883a7bcabd",
        "colab": {}
      },
      "source": [
        "#average='macro'\n",
        "metrics.recall_score(y_test_mlb, y_pred_clf,average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/dsn/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4586565631405319"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLwxl8HkbWYU",
        "colab_type": "code",
        "outputId": "e8624bf4-aa46-4b93-d42f-d83bd44c9cb6",
        "colab": {}
      },
      "source": [
        "#average='weighted'\n",
        "metrics.recall_score(y_test_mlb, y_pred_clf,average='weighted')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7765720580307967"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U2vyVyMbWYW",
        "colab_type": "code",
        "outputId": "e5b50b64-eacb-4d13-b795-2d3fbefc128b",
        "colab": {}
      },
      "source": [
        "metrics.multilabel_confusion_matrix(y_test_mlb, y_pred_clf,sample_weight=None,labels=None,samplewise=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[   0,    0],\n",
              "        [   0, 2500]],\n",
              "\n",
              "       [[2429,    3],\n",
              "        [  62,    6]],\n",
              "\n",
              "       [[2500,    0],\n",
              "        [   0,    0]],\n",
              "\n",
              "       [[1827,   48],\n",
              "        [ 310,  315]],\n",
              "\n",
              "       [[1760,   93],\n",
              "        [ 406,  241]],\n",
              "\n",
              "       [[ 967,  235],\n",
              "        [ 307,  991]],\n",
              "\n",
              "       [[2113,   24],\n",
              "        [ 233,  130]],\n",
              "\n",
              "       [[1536,  137],\n",
              "        [ 408,  419]],\n",
              "\n",
              "       [[1869,   35],\n",
              "        [ 331,  265]],\n",
              "\n",
              "       [[1878,   51],\n",
              "        [ 399,  172]],\n",
              "\n",
              "       [[2487,    0],\n",
              "        [  11,    2]],\n",
              "\n",
              "       [[2475,    0],\n",
              "        [  24,    1]],\n",
              "\n",
              "       [[1096,  212],\n",
              "        [ 316,  876]],\n",
              "\n",
              "       [[2448,    0],\n",
              "        [  40,   12]],\n",
              "\n",
              "       [[2284,   14],\n",
              "        [ 143,   59]],\n",
              "\n",
              "       [[2399,    3],\n",
              "        [  89,    9]],\n",
              "\n",
              "       [[2087,    9],\n",
              "        [ 203,  201]],\n",
              "\n",
              "       [[2463,    0],\n",
              "        [  37,    0]],\n",
              "\n",
              "       [[2499,    1],\n",
              "        [   0,    0]],\n",
              "\n",
              "       [[2449,    1],\n",
              "        [  40,   10]],\n",
              "\n",
              "       [[2289,    9],\n",
              "        [ 174,   28]],\n",
              "\n",
              "       [[2430,    1],\n",
              "        [  51,   18]],\n",
              "\n",
              "       [[2487,    1],\n",
              "        [  12,    0]],\n",
              "\n",
              "       [[2369,    4],\n",
              "        [  96,   31]],\n",
              "\n",
              "       [[2472,    0],\n",
              "        [  22,    6]],\n",
              "\n",
              "       [[1605,  101],\n",
              "        [ 396,  398]],\n",
              "\n",
              "       [[1478,  139],\n",
              "        [ 354,  529]],\n",
              "\n",
              "       [[1581,   93],\n",
              "        [ 485,  341]],\n",
              "\n",
              "       [[2433,    1],\n",
              "        [  54,   12]],\n",
              "\n",
              "       [[   0,    0],\n",
              "        [   0, 2500]],\n",
              "\n",
              "       [[2365,    6],\n",
              "        [ 113,   16]],\n",
              "\n",
              "       [[1006,  261],\n",
              "        [ 336,  897]],\n",
              "\n",
              "       [[1168,  135],\n",
              "        [ 443,  754]],\n",
              "\n",
              "       [[   0,    0],\n",
              "        [   0, 2500]],\n",
              "\n",
              "       [[1378,  113],\n",
              "        [ 420,  589]],\n",
              "\n",
              "       [[1229,  172],\n",
              "        [ 462,  637]],\n",
              "\n",
              "       [[1256,  170],\n",
              "        [ 299,  775]],\n",
              "\n",
              "       [[   6,   67],\n",
              "        [   7, 2420]],\n",
              "\n",
              "       [[1508,  104],\n",
              "        [ 500,  388]],\n",
              "\n",
              "       [[   0,    0],\n",
              "        [   0, 2500]],\n",
              "\n",
              "       [[   0,    0],\n",
              "        [   0, 2500]],\n",
              "\n",
              "       [[   0,   18],\n",
              "        [   1, 2481]],\n",
              "\n",
              "       [[ 597,  331],\n",
              "        [ 204, 1368]],\n",
              "\n",
              "       [[2198,   13],\n",
              "        [ 225,   64]],\n",
              "\n",
              "       [[2353,    8],\n",
              "        [ 106,   33]],\n",
              "\n",
              "       [[  46,  163],\n",
              "        [  25, 2266]],\n",
              "\n",
              "       [[ 355,  288],\n",
              "        [ 156, 1701]],\n",
              "\n",
              "       [[1637,  105],\n",
              "        [ 385,  373]],\n",
              "\n",
              "       [[1472,  111],\n",
              "        [ 463,  454]],\n",
              "\n",
              "       [[2449,    0],\n",
              "        [  41,   10]],\n",
              "\n",
              "       [[2491,    1],\n",
              "        [   8,    0]],\n",
              "\n",
              "       [[1760,   68],\n",
              "        [ 336,  336]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5i2JpFxbWYa",
        "colab_type": "text"
      },
      "source": [
        "#### 10.\t Print true label and predicted label for any five examples (7.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ySWXaVYbWYb",
        "colab_type": "code",
        "outputId": "3516a9e3-f32c-430d-e61b-f2686796969f",
        "colab": {}
      },
      "source": [
        "y_pred_clf[100:105]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
              "        0, 0, 1, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFyH8tI5bWYg",
        "colab_type": "code",
        "outputId": "34d5376a-89b2-40ca-fbc0-d790d2efc623",
        "colab": {}
      },
      "source": [
        "y_test_mlb[100:105]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "        0, 1, 1, 1, 1, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "        0, 1, 1, 0, 1, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "        0, 1, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnfAuajzbWYj",
        "colab_type": "code",
        "outputId": "e1b41458-3a6f-4776-e867-284a85b73cc3",
        "colab": {}
      },
      "source": [
        "labels.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['indUnk', 'Sagittarius', '24', 'female', '34', '27', 'Education', 'Aquarius', '36', 'Aries', 'Fashion', 'male', '15', 'Libra', 'Student', 'Capricorn', '16', '35', 'Technology', 'Taurus', 'Pisces', 'Sports-Recreation', '17', 'Leo', 'Scorpio', '39', 'Virgo', '26', 'Engineering', '23', '13', 'Cancer', '14', '25', 'Science', 'Communications-Media', 'Gemini', 'Marketing', 'Automotive', 'InvestmentBanking', '33', 'Banking', '42', 'Consulting', '40', 'Internet', 'BusinessServices', 'Law', 'Arts', '37', '38', '44', 'Museums-Libraries', '43', 'Non-Profit', 'LawEnforcement-Security', '45', '41', 'HumanResources', 'Publishing', 'Religion', '46', 'Accounting'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWZFTWzEbWYm",
        "colab_type": "code",
        "outputId": "0f73af98-67f2-4836-820c-52369c8a3de4",
        "colab": {}
      },
      "source": [
        "y_test_mlb.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2500, 52)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZVtQ851bWYs",
        "colab_type": "code",
        "outputId": "32174113-1bf8-4093-fe79-0791dcdaeee4",
        "colab": {}
      },
      "source": [
        "labels_list = list(labels.keys())[:52]\n",
        "labels_list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['indUnk',\n",
              " 'Sagittarius',\n",
              " '24',\n",
              " 'female',\n",
              " '34',\n",
              " '27',\n",
              " 'Education',\n",
              " 'Aquarius',\n",
              " '36',\n",
              " 'Aries',\n",
              " 'Fashion',\n",
              " 'male',\n",
              " '15',\n",
              " 'Libra',\n",
              " 'Student',\n",
              " 'Capricorn',\n",
              " '16',\n",
              " '35',\n",
              " 'Technology',\n",
              " 'Taurus',\n",
              " 'Pisces',\n",
              " 'Sports-Recreation',\n",
              " '17',\n",
              " 'Leo',\n",
              " 'Scorpio',\n",
              " '39',\n",
              " 'Virgo',\n",
              " '26',\n",
              " 'Engineering',\n",
              " '23',\n",
              " '13',\n",
              " 'Cancer',\n",
              " '14',\n",
              " '25',\n",
              " 'Science',\n",
              " 'Communications-Media',\n",
              " 'Gemini',\n",
              " 'Marketing',\n",
              " 'Automotive',\n",
              " 'InvestmentBanking',\n",
              " '33',\n",
              " 'Banking',\n",
              " '42',\n",
              " 'Consulting',\n",
              " '40',\n",
              " 'Internet',\n",
              " 'BusinessServices',\n",
              " 'Law',\n",
              " 'Arts',\n",
              " '37',\n",
              " '38',\n",
              " '44']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DkrSuYsbWYy",
        "colab_type": "code",
        "outputId": "bd1a5cf4-3289-4e3e-dacd-2287dc88e03c",
        "colab": {}
      },
      "source": [
        "#marking the labels \n",
        "\n",
        "import numpy as np\n",
        "count=0\n",
        "for i in range(5):\n",
        "    pred_list = []\n",
        "    true_list = []\n",
        "    values = y_pred_clf[100+count:100+count+1]\n",
        "    searchval = 1\n",
        "    pred_indices = np.where(values == searchval)[1] \n",
        "    for i in pred_indices:\n",
        "        pred_list.append(labels_list[i])\n",
        "    values = y_test_mlb[100+count:100+count+1]\n",
        "    print(\"predicted labels for {}th record : {} \\n\".format(100+count,pred_list))\n",
        "\n",
        "    true_indices = np.where(values == searchval)[1]\n",
        "    for k in true_indices:\n",
        "        true_list.append(labels_list[k])\n",
        "    print(\"true labels for {}th record : {}\\n\".format(100+count,true_list))\n",
        "    print(\"---------------------------------------------------------------\")\n",
        "    count+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted labels for 100th record : ['indUnk', '27', '15', 'Virgo', '23', 'Cancer', '25', 'Communications-Media', 'Gemini', 'Marketing', 'InvestmentBanking', '33', 'Banking', '42', 'Internet', 'BusinessServices'] \n",
            "\n",
            "true labels for 100th record : ['indUnk', '27', 'Aquarius', '15', 'Virgo', '23', 'Cancer', '25', 'Communications-Media', 'Gemini', 'Marketing', 'InvestmentBanking', '33', 'Banking', '42', 'Internet', 'BusinessServices', '44']\n",
            "\n",
            "---------------------------------------------------------------\n",
            "predicted labels for 101th record : ['indUnk', '15', '23', '25', 'Communications-Media', 'Gemini', 'Marketing', 'InvestmentBanking', '33', 'Banking', '42', 'Internet', 'BusinessServices'] \n",
            "\n",
            "true labels for 101th record : ['indUnk', '34', 'Education', '39', '26', '23', '14', '25', 'Science', 'Communications-Media', 'Marketing', 'Automotive', 'InvestmentBanking', '33', 'Banking', 'Internet', 'BusinessServices', 'Law', 'Arts']\n",
            "\n",
            "---------------------------------------------------------------\n",
            "predicted labels for 102th record : ['indUnk', '23', '25', 'Marketing', 'InvestmentBanking', '33', 'Banking', '42', 'Internet', 'BusinessServices'] \n",
            "\n",
            "true labels for 102th record : ['indUnk', '34', 'Aries', 'Virgo', '26', '23', '14', '25', 'Science', 'Marketing', 'Automotive', 'InvestmentBanking', '33', 'Banking', 'Internet', 'BusinessServices', 'Arts']\n",
            "\n",
            "---------------------------------------------------------------\n",
            "predicted labels for 103th record : ['indUnk', '27', '36', '15', '16', '23', '25', 'Gemini', 'Marketing', 'InvestmentBanking', '33', 'Banking', '42', 'Internet', 'BusinessServices'] \n",
            "\n",
            "true labels for 103th record : ['indUnk', '27', '36', '15', '16', '23', '25', 'Gemini', 'Marketing', 'InvestmentBanking', '33', 'Banking', '42', 'Internet', 'BusinessServices']\n",
            "\n",
            "---------------------------------------------------------------\n",
            "predicted labels for 104th record : ['indUnk', '27', '36', '16', 'Pisces', '23', '25', 'Marketing', 'InvestmentBanking', '33', 'Banking', '42', 'BusinessServices', 'Law'] \n",
            "\n",
            "true labels for 104th record : ['indUnk', '34', '27', 'Pisces', '26', '23', '13', '14', '25', 'Science', 'Marketing', 'Automotive', 'InvestmentBanking', '33', 'Banking', 'Internet']\n",
            "\n",
            "---------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPuP-Q5TbWY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}